# Artificial Neural Networks labs

### Lab 1: Learning and generalisation in feed-forward networks from perceptron learning to backprop

[Lab instructions](https://github.com/tgll/neuralnetworks-labs/blob/master/lab1/annda_lab1.pdf)

Visualise the [code](https://nbviewer.jupyter.org/github/tgll/neuralnetworks-labs/blob/master/lab1/lab1-perceptron.ipynb) with Nbviewer

#### In this lab:

Perceptron learning VS delta learning
![Perceptron](img/perceptrondeltalearning2.png)

Batch learning VS sequential learning
![Gradient1](img/GradientDescent.png)
![Gradient2](img/GradientDescent_B.png)

Optimizers
![Optimizers1](img/GradientDescentC.png)

Optimizers Mindmap
![Optimizers2](img/OptimizersMindmap.png)
(full mindmap [here](https://coggle.it/diagram/XbOLnq4hARCMvhuY/t/cognitive-sciences-related-knowledge))

Linearly separable data VS not linearly separable data
![Optimizers2](img_results/1.2_Q2batchclassif.png)
![Optimizers2](img_results/2.1_classification.png)

------

### See the other labs [here](https://github.com/tgll/neuralnetworks-labs)